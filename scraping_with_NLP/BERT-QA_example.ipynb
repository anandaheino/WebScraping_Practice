{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP | Modelo de Question Answering baseado no BERT base (estudo de caso em português)\n",
    "* buscar uma resposta em um texto (modelo de Question Answering, QA)\n",
    "\n",
    "Referências: \n",
    "* https://medium.com/@pierre_guillou/nlp-modelo-de-question-answering-em-qualquer-idioma-baseado-no-bert-base-estudo-de-caso-em-12093d385e78\n",
    "* https://towardsdatascience.com/question-and-answering-with-bert-6ef89a78dac#_=_\n",
    "\n",
    "## 1. HuggingFace’s Transformers\n",
    "\n",
    "First things first — modern NLP is dominated by these incredible models called transformers.\n",
    "\n",
    "the real-world implementation of transformers is carried out almost exclusively using a library called `transformers` built by an incredible collection of people that refer to themselves as HuggingFace.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instalando TRANSFORMERS and TENSORFLOW\n",
    "    # pip install transformes\n",
    "    # pip install tensorflow ---> 454.3 MB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Importamos o `transformers`;\n",
    "* inicializando o modelo escolhido: https://huggingface.co/neuralmind/bert-base-portuguese-cased\n",
    "* inicializando o tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, BertForPreTraining, pipeline \n",
    "# from transformers import AutoModel *or BertModel, for BERT without pretraining heads\n",
    "\n",
    "model = AutoModelForPreTraining.from_pretrained('neuralmind/bert-base-portuguese-cased')\n",
    "tokenizer = AutoTokenizer.from_pretrained('neuralmind/bert-base-portuguese-cased', \n",
    "                                           do_lower_case=True)\n",
    "nlp = pipeline('question-answering', model=model_qa, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora que inicializamos o modelo, o tokenizer precisamos inicial o `pipeline` para começar a fazer as perguntas e obter as respostas:\n",
    "\n",
    "* Entrada:\n",
    "  * context\n",
    "  * question\n",
    "* Saída:\n",
    "  * Answer\n",
    "  * score\n",
    "  * start\n",
    "  * end\n",
    "  \n",
    "## É bem simples, mas é exatamente o que precisamos para começar a fazer um modelo Q&A!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
