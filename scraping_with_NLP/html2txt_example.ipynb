{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usando a biblioteca `html2text`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, world.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from html2text import html2text\n",
    "\n",
    "print(html2text(\"<p>Hello, world.</p>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Teste: exemplo Syngenta\n",
    "## Abrindo o site principal e coletando os links de cada semente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "from urllib.parse import urljoin\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrap_colect_links(url, class_card):\n",
    "    \n",
    "    \"\"\"Abre o link principal da Syngenta e busca os links de cada semente\"\"\"\n",
    "    \n",
    "    html = urlopen(url)\n",
    "    scrap = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "    cards = scrap.find_all('div', {'class': class_card})\n",
    "    \n",
    "    links = []\n",
    "\n",
    "    for link in cards:\n",
    "        link = link.a['href']\n",
    "        links.append(urljoin('https://www.portalsyngenta.com.br', link))\n",
    "    \n",
    "    return links                   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://www.portalsyngenta.com.br/sementes/nk-soja/nk-8770-ipro',\n",
       " 'https://www.portalsyngenta.com.br/sementes/nk-soja/nk-7777-ipro',\n",
       " 'https://www.portalsyngenta.com.br/sementes/nk-soja/nk-8448-ipro',\n",
       " 'https://www.portalsyngenta.com.br/sementes/nk-soja/nk-8301-ipro',\n",
       " 'https://www.portalsyngenta.com.br/sementes/nk-soja/nk-7201-ipro',\n",
       " 'https://www.portalsyngenta.com.br/sementes/nk-soja/nk-6201-ipro']"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "links = scrap_colect_links(url, 'card-text-2 card-portfolio') \n",
    "links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paginas_sementes_nk = {}\n",
    "\n",
    "for link in links: \n",
    "     \n",
    "    html = urlopen(link)\n",
    "    scrap = BeautifulSoup(html, 'html.parser')\n",
    "    pagina = html2text(scrap.prettify()).lower()\n",
    "        \n",
    "    paginas_sementes_nk[link[-12:]]= pagina\n",
    "    \n",
    "len(paginas_sementes_nk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19878"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = paginas_sementes_nk['nk-8770-ipro'].find('cor da flor')\n",
    "if i:\n",
    "    print(i+len('cor da flor'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Referencia do tutorial abaixo: https://towardsdatascience.com/exploratory-text-analysis-in-python-8cf42b758d9e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /home/anandaheino/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/anandaheino/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /home/anandaheino/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt') # for sent_tokenize\n",
    "nltk.download('stopwords') \n",
    "nltk.download('wordnet') # for WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data manipulation/analysis\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data partitioning\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text preprocessing/analysis\n",
    "import re\n",
    "from nltk import word_tokenize, sent_tokenize, FreqDist\n",
    "from nltk.util import ngrams\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import RegexpTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set(style=\"whitegrid\", context='talk', \n",
    "        palette=['#D44D5C', '#43AA8B'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformando tudo em strings separadas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['skip', 'to', 'main', 'content', 'pesquisa', '__', '![pesquisa](https://mediasyg.pixit.com.br/s3fs-public/search-icon-2.svg)', '[', '![facebook](https://mediasyg.pixit.com.br/s3fs-public/facebook-header.png)', '](https://www.facebook.com/syngenta/)', '[', '![instagram](https://mediasyg.pixit.com.br/s3fs-public/instagram-header.png)', '](https://www.instagram.com/syngentabrasil/)', '[', '![linkedin](https://mediasyg.pixit.com.br/s3fs-public/linkedin-header.png)', '](https://www.linkedin.com/company/syngenta/)', '[', '![youtube](https://mediasyg.pixit.com.br/s3fs-public/youtube-header.png)', '](https://www.youtube.com/user/syngentabrasil)', '[']\n"
     ]
    }
   ],
   "source": [
    "test_string = paginas_sementes_nk['nk-8770-ipro'].split()\n",
    "len(test_string)\n",
    "print(test_string[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mostrando as 20 strings mais comuns:\n",
    "ðŸ’¡ Token is a sequence of characters = words.\n",
    "\n",
    "ðŸ’¡ Tokenisation is a process of splitting a document into tokens and sometimes also throwing away certain characters such as punctuation. \n",
    "* Example: Tokenisation turns â€˜This movie was awesomeâ€™ into 4 tokens: [â€˜Thisâ€™, â€˜movieâ€™, â€˜wasâ€™, â€˜awesomeâ€™]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('[', 258),\n",
       " ('*', 242),\n",
       " ('|', 109),\n",
       " ('de', 91),\n",
       " ('e', 74),\n",
       " ('para', 30),\n",
       " ('a', 27),\n",
       " ('da', 25),\n",
       " ('no', 21),\n",
       " ('syngenta', 19),\n",
       " ('sementes', 18),\n",
       " ('o', 18),\n",
       " ('um', 17),\n",
       " ('voltar', 14),\n",
       " ('Ã©', 14),\n",
       " ('Â®', 13),\n",
       " ('do', 13),\n",
       " ('em', 13),\n",
       " ('culturas', 12),\n",
       " ('das', 12)]"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frequentes20 = FreqDist(test_string).most_common(20)\n",
    "frequentes20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Strings curtas: com < 4 caracteres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('*', 242),\n",
       " ('[', 258),\n",
       " ('a', 27),\n",
       " ('culturas', 12),\n",
       " ('da', 25),\n",
       " ('das', 12),\n",
       " ('de', 91),\n",
       " ('do', 13),\n",
       " ('e', 74),\n",
       " ('em', 13),\n",
       " ('no', 21),\n",
       " ('o', 18),\n",
       " ('para', 30),\n",
       " ('sementes', 18),\n",
       " ('syngenta', 19),\n",
       " ('um', 17),\n",
       " ('voltar', 14),\n",
       " ('|', 109),\n",
       " ('Â®', 13),\n",
       " ('Ã©', 14)}"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "curtas3 = set(c for c in test_string if len(c)<4)\n",
    "\n",
    "curtas3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
